\documentclass{article}
\usepackage[english]{babel}
\usepackage{mathtools}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{url}
\usepackage{xcolor}

\title{
  A foundational model for types \\
  \large Presentation notes  
}
\author{
  Enríquez Ballester, Adrián 
  \and
  Isasa Martín, Carlos Ignacio
  \and 
  Mata Aguilar, Luis
  \and 
  Bak, Mieczyslaw
}

\begin{document}
\maketitle

\section*{Foundational Proof-Carrying Code}

In 1996, a Gorge Necula's paper \cite{necula:pcc} 
introduces the idea of Proof-Carrying code:

\begin{enumerate}
  \item \textbf{Code producer}: generates an executable 
    together with a proof (certificate) that the program 
    adheres to some safety policy.
  \item \textbf{Code consumer}: receives some untrusted 
    executable with its proof and can validate it before 
    running.
\end{enumerate}

The idea is not about cryptography, but about type systems 
for machine code (i.e. a deductive system defined over 
machine instructions which is proved to guarantee or 
preserve some properties).

Based on that work, Andrew W. Appel, known for being a 
major contributor of the StandardML compiler, starts its 
research in Foundational Proof-Carrying Code, which he 
defines as \cite{appel:fpcc}:

\begin{center}
\textit{
  A framework for mechanical verification of safety 
  properties of machine language...
}
\end{center}

until here it is Proof-Carrying Code

\begin{center}
\textit{
  ...with the smallest possible runtime and verifier.
}
\end{center}

and this last part is the Foundational one. 

The drawback that he sees for Proof-Carrying Code as 
initially described is that it is ad-hoc for each specific 
case, and built-in type rules and lemmas must be a trusted 
part because they are human-verified. Foundational 
Proof-Carrying Code relies on a primitive logic (e.g. high 
order with some arithmetic axioms) which is powerful 
enough to encode the required type system and lemmas. It 
means that they are instead proved in this foundational 
logic. With this, the aim is to make the trusted part as 
small as possible.

At that moment, they chose Twelf for defining the logic and
the required encodings, this is just an example to 
illustrate how it looks like:

\begin{verbatim}
                      tp   : type.
                      tm   : tp -> type.
                      num  : tp.
                      pair : tp -> tp -> tp.
                      
\end{verbatim}

One of the parts that must be modeled within this logic is 
the target machine architecture. The behavior (i.e. 
semantic) and encoding (i.e. syntax) of machine 
instructions must be defined, and they believe that it is
possible for every usual architecture in a similar way 
(i.e. as a step relation $(r,m) \mapsto (r',m')$ where $r$ 
and $r'$ are states of the register bank and $m$ and $m'$ 
of the memory). For example, they encoded the SPARK 
architecture by means of 1035 Twelf LOC for the syntactic 
part, generated with a 151 LOC of a higher level language 
due to redundancies, and 600 Twelf LOC for the semantic 
one. This is an example of an \textit{add} instruction 
encoding:

\begin{align*}
  \mathsf{add}(i, j, k) = \;\;\;\;&\\ 
    \lambda r,m,r',m'.\;&r'(i) = r(j) + r(k) \\
      &\land (\forall x \neq i.\;r'(x) = r(x)) \\
      &\land m' = m \\
\end{align*}

Safety requirements can be specified in the syntax and 
semantics themselves, by making the step relation 
deliberately partial or by making some syntax forbidden 
just by not defining it. This is a dumb example of the 
previous instruction to be not allowed for a certain 
register:

\begin{align*}
  \mathsf{add}(i, j, k) = \;\;\;\;&\\ 
    \lambda r,m,r',m'.\;&r'(i) = r(j) + r(k) \\
      &\land (\forall x \neq i.\;r'(x) = r(x)) \\
      &\land m' = m \\
      &\land i \neq 42 \\
\end{align*}

Now, for proving the adherence to the safety requirements,
an appropriate type system must be defined for the machine
instructions and, as they follow a so called semantic 
approach, it requires the following to be encoded in the 
foundational logic as proofs, not as built-ins:

\begin{itemize}
  \item Type judgements are assigned a truth value.
  \item If the premise judgements are true, then the 
    conclusion judgement must be true.
  \item If a type judgement is true, then it corresponds
    to a safe state.
\end{itemize}

One of the most challenging parts has been to find an 
appropriate model for encoding type systems. Its first 
approach \cite{appel:fpcc:semantic} was to model types as 
sets of values, and model values in a direct way like a 
pair consisting of the memory and a memory address, but 
they encounter some limitations:

\begin{itemize}
  \item They were unable to model mutable fields.
  \item They were unable to model certain kinds of 
    recursive datatype definitions.
\end{itemize}

Their second approach \cite{appel:fpcc:indexed} was to 
model types as sets of pairs $\langle k, v \rangle$ where 
$k$ is an approximation index and $v$ a value. The 
judgement $\langle k, v \rangle \in \tau$ means informally 
that $v$ can be considered to have type $\tau$ for a 
program running for less than $k$ steps. This model solved 
their problem with recursion, but the one with mutable 
fields remained. A PhD thesis of a student of A.W. Appel 
offered later a model which solved also that problem. 

\section*{Interlude}

Ten years later, A.W. Appel says that now it is practical 
to prove safety and correctness with type systems for 
source code instead of machine code and they are 
trustworthy if compiled with a formally verified compiler
\cite{appel:fpcc:compilers}, so he is now involved in 
projects of this kind (e.g. CertiCoq 
\cite{website:certicoq}, CompCert \cite{website:compcert}, 
CertiKOS \cite{website:certikos}).

However, although the results of this research seem to 
have not so much practical interest nowadays, we want to
show how they encoded type systems in a foundational way, 
which is not only applicable to type systems for machine 
code as they show for example with a usual typed lambda 
calculus.

\section*{A foundational model for types}

Why a semantic model and not just to be able to define a set
of rules? Because in this way the rules are required to have valid 
proofs and do not require a trusted party.

They have been experimenting with usual type systems for functional 
and imperative languages, and even for type systems designed for 
machine instructions. Its first attempt was to model types directly 
as sets of values, defined according to each use case, but they 
would not be able to model datatypes with contravariant recursion
and mutable fields.

Its second approach, which we are going to show briefly, was also 
lacking some features, but they focused on improving the recursion 
support.

\subsection*{Indexed model for types}

A type is modeled as a set of pairs of the form 
$\langle k, v \rangle$ where $k$ is a nonnegative integer called 
index, $v$ is a value and it is closed under decreasing index 
(i.e.whenever $\langle k, v \rangle$ belongs to a type $\tau$, 
we also have that $\langle j, v \rangle$ belongs to 
$\tau$ for all $j \geq k$).

We write $e :_k \tau$ if $e \rightarrow^j v$ for some $j < k$ 
implies $\langle k - j, v\rangle \in \tau$, where $\rightarrow$ 
is the step relation given by the corresponding small step 
semantics.

In each case, the type sets must be defined, but some conventional 
ones which do not depend on a specific use case are for example 

\begin{align*}
  \bot &\equiv \{\} \\ 
  \top &\equiv \{ \langle k, v \rangle\;|\; k \geq 0 \}
\end{align*}

Type environment and states are modeled respectively as mappings 
from variables to types and variables to values. The consistency 
of a state $\sigma$ and environment $\Gamma$ is written 
$\sigma :_k \Gamma$ and defined as $\forall x \in dom(\Gamma)$
we have $\sigma(x) :_k \Gamma(x)$.

The entailment relation $\Gamma \models_k e$ is the semantic 
counterpart of a type judgement and means 
$\sigma(e):_k \alpha$ for all state $\sigma$ consistent with 
$\Gamma$, where $\sigma(e)$ is the result of replacing all the 
free variables of $e$ with their values in $\sigma$. Also, 
$\Gamma \models e : \alpha$ means 
$\Gamma \models e :_k \alpha$ for all $k \geq 0$.

\subsection*{Lambda calculus example}

As an example of type system modeled in this way, they consider 
a lambda calculus with pairs and the constant $0$:

$$
  e \Coloneqq x 
      \;|\; 0 
      \;|\; \langle e_1, e_2 \rangle 
      \;|\; \pi_1(e) 
      \;|\; \pi_2(e)
      \;|\; \lambda x. e 
      \;|\; e_1\;e_2
$$

Values are the constant $0$, a closed abstraction 
$\lambda x.e$ (i.e. no free variables) and a pair of values 
$\langle v_1, v_2 \rangle$.

For the semantics, they consider a pretty standard small step and 
safeness is to not reach a stuck term.

\textcolor{red}{
  SMALL STEP RULES: PAPER OF INDEXED MODEL FIGURE 1
}

The defined type sets which they define for this use case are,
together with $\bot$ and $\top$:

\begin{align*}
  \mathsf{int} &\equiv \{ \langle k, 0 \rangle \;|\; 
    \forall k.\;k \geq 0 \} \\
  \tau_1 \times \tau_2 &\equiv 
    \{ \langle k, \langle v_1, v_2 \rangle \rangle \;|\; 
          \forall j.\;j < k \land
          \langle j, v_1\rangle \in \tau_1  \land 
          \langle j, v_2 \rangle \in \tau_2
    \} \\
  \alpha \rightarrow \tau &\equiv 
    \{ \langle k, \lambda x. e \rangle \;|\; 
        \forall j.\;j < k \land
        \langle j, v \rangle \in \alpha \implies
          e[v/x] :_j \tau
    \} \\ 
  \mu F &\equiv 
    \{ \langle k, v \rangle \;|\; \langle k, v \rangle \in 
      F^{k + 1}(\bot)
    \}
\end{align*}

\textcolor{red}{
  DISCUSS WHY IN THIS CASE $\models e : \alpha$ IS SAFE.
}

\textcolor{red}{
  THE RULES OF THE TYPE SYSTEM: PAPER OF INDEXED MODEL FIGURE 2
}

\textcolor{red}{
  SHOW THAT THE AXIOMS FOLLOW FROM THE DEFINITIONS
}

\textcolor{red}{
  SHOW HOW SOME OF THE RULES AND TYPES ALSO CAN BE PROVED 
  (SELECT FROM ABSTRACTION, APPLICATION, PAIR, PROJECTION...)
}

\textcolor{red}{
  DISCUSS BRIEFLY ABOUT RECURSION. IT IS THE MOST DIFFICULT PART 
  BUT ALSO IMPORTANT BECAUSE IT MOTIVATES THE INDICES IN THIS MODEL
}

\section*{Conclusion}

Foundational Proof Carrying Code seems to be out of interest 
nowadays, but it motivated a research about modeling type systems 
in a foundational way.

In that research, to model types directly as sets of values was
not enough for some features such as contravariant recursive
definitions and mutable fields.

A more sophisticated model, which we are presenting here, allowed 
them to model better recursion, and further research seems to have 
solved even the problem they had about mutable fields.

These results may help in studying type systems from the point of 
view of foundational mathematics and also where the introduction 
of rules must be correct within a logic in a machine-checkable 
way.

\textcolor{red}{
  THESE ARE SOME OF THE FEATURES THAT THEY SUCCESFULLY 
  ENCODED WITH THIS MODEL, I PUT THEM HERE AS A DRAFT:
}

\begin{itemize}
  \item Usual features of a functional language.
  \item Usual features of an imperative and OO language.
  \item Specific type systems for machine instructions.
  \item Recursive datatype definitions.
  \item polymorphism, universal and existential quantification.
\end{itemize}

\bibliography{refs}{}
\bibliographystyle{plain}

\end{document}
